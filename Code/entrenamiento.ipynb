{"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MZ1zM7hXQMQ","executionInfo":{"status":"ok","timestamp":1692474889839,"user_tz":300,"elapsed":12507,"user":{"displayName":"Daniel Duque","userId":"09388715961090398566"}},"outputId":"fc5caa49-3a0e-414e-dcdb-cdbf94863bb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"BV4BHuUMXiDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3pVw6P3UVdM5","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"error","timestamp":1696563462873,"user_tz":300,"elapsed":4070,"user":{"displayName":"Daniel Duque","userId":"09388715961090398566"}},"outputId":"1939bb92-d3dc-4230-ad3b-d56487a4d0bb"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e39cbaac966d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on aug 18 2023\n","\n","@author: Daniel Duque Lozano\n","\n","This code is adapted from https://github.com/ofiscal/contract-transparency---copia\n","\"\"\"\n","import tensorflow as tf\n","from transformers import TFAutoModelForSequenceClassification,AutoTokenizer\n","from transformers.onnx import config\n","import pandas as pd\n","\n","import numpy as np\n","from sklearn.metrics import r2_score\n","\n","\n","#We start a tf session that is prone to run eagerly\n","tf.config.run_functions_eagerly(True)\n","\n","path_to_data=r\"/content/drive/MyDrive/Observatorio Fiscal Conjunto/Gasto Público/SAPO/Otros/datos_paraguay/datos/records.csv\"\n","#This parameters are used to normalize, however they may be misleading as they\n","#Aren´t really means or standard deviation in any actual structure\n","mean=1e+12\n","ssd=1e+17\n","\n","\n","entrenar=\"TR\"\n","setsize=20000\n","#we run a subset of the 2023 dataset from contracts in paraguay open data\n","#we can find the source here https://www.contrataciones.gov.py/datos/api/v3/doc/\n","#As we need complet information to train we drop empty values and normalize\n","data=pd.read_csv(path_to_data,nrows=setsize)\n","data=data.dropna(subset=\"compiledRelease/tender/value/amount\")\n","data=data.dropna(subset=\"compiledRelease/planning/budget/description\")\n","data[\"valor norm\"]=data[\"compiledRelease/tender/value/amount\"].apply(\n","    lambda x:(x-mean)/ssd\n",")\n","\n","\n","\n","#We use pre trained weights for bert multilingual case, we use the next\n","#code in order to load those weights, tokenize our data in bert fomat an organ-\n","#ize it in a way it makes sense for the training proces\n","checkpoint=\"bert-base-multilingual-cased\"\n","tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n","pre_token=data[\"compiledRelease/planning/budget/description\"].tolist()\n","tokens=tokenizer(pre_token,padding=True,return_tensors=\"np\")\n","tokenized_data = dict(tokens)\n","labels = np.array(data[\"valor norm\"])\n","model = TFAutoModelForSequenceClassification.from_pretrained(\n","    checkpoint,num_labels=1)\n","\n","\"\"\"\n","The next part is the portion of the model that we stablish, we use adam optimiz-\n","er based on gradient descent aplying adaptative momentes, pretty usefull when\n","trying to reduce convergence to local minimums\n","\n","\"\"\"\n","adamizer=tf.keras.optimizers.Adam(\n","    learning_rate=0.00001,\n","    beta_1=0.9,\n","    beta_2=0.999,\n","    epsilon=1e-07,\n","\n",")\n","\n","model.compile(optimizer=adamizer)\n","#The next part of de model stablish that the loaded weights won´t be trained,\n","#just the later stage that connects the result.\n","model.layers[0].trainable = False\n","\n","model.fit(x=tokenized_data,y=labels,batch_size=4, epochs=1, validation_split=0.2)\n","results=model.predict(tokenized_data).logits\n","resultados=pd.DataFrame(results)\n","\n","data[\"Precio Predecido\"]=resultados[0].apply(lambda x: (x*ssd)+mean)\n","\n","data.to_excel(\"/content/drive/MyDrive/Observatorio Fiscal Conjunto/Gasto Público/SAPO/Otros/datos_paraguay/datos/resultshugging.xlsx\")\n","\n","\"\"\"\n","As ending remarks of this script:\n","This is just a trial that shows the ability to train in a limited/small environ-\n","ment.\n","\n","\"\"\"\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"Itw2VT2jEDqJ"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1W5Cg-B3Ga78LwkBpMX0dQbmEpT_FpHyX","authorship_tag":"ABX9TyO4FcswXQMWOJKfbaPno6if"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}